#!/usr/bin/env python3
"""
VMEvalKit Analysis Tool

Analyzes evaluation results to show model performance by domain and overall rankings.
Only scores 4 and 5 are considered "correct" (successful).

Usage:
    python analysis/plot.py --eval-folder data/evaluations/human-eval/
    python analysis/plot.py --eval-folder data/evaluations/gpt4o-eval/
"""

import os
import json
import argparse
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from collections import defaultdict
import numpy as np
from datetime import datetime
import matplotlib.colors as mcolors

# Set up plotting style with sophisticated color scheme
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("deep")

# Define color schemes
MAIN_PALETTE = sns.color_palette("deep", 10)  # For distinct models
SUCCESS_CMAP = plt.cm.RdYlGn  # Red-Yellow-Green for success rates
DOMAIN_COLORS = {
    'chess': '#2E86AB',     # Deep blue
    'maze': '#A23B72',      # Purple
    'raven': '#F18F01',     # Orange
    'rotation': '#C73E1D',  # Red
    'sudoku': '#6A994E'     # Green
}

# Define the domain order
DOMAIN_ORDER = ['sudoku', 'raven', 'maze', 'chess', 'rotation']

# Model signature colors (predefined for consistency)
MODEL_COLORS = {
    'openai-sora-2': '#1f77b4',        # Blue
    'veo-3.0-generate': '#ff7f0e',     # Orange  
    'veo-3.1-720p': '#2ca02c',         # Green
    'runway-gen4-turbo': '#d62728',    # Red
    'wavespeed-wan-2.2-i2v-720p': '#9467bd',  # Purple
    'luma-ray-2': '#8c564b'            # Brown
}

def load_evaluation_data(eval_folder: Path) -> list:
    """Load all evaluation JSON files from the specified folder."""
    evaluations = []
    
    for json_file in eval_folder.rglob("*.json"):
        try:
            with open(json_file, 'r') as f:
                data = json.load(f)
            
            # Extract relevant information
            if "metadata" in data and "result" in data:
                eval_data = {
                    "model_name": data["metadata"].get("model_name", "unknown"),
                    "task_type": data["metadata"].get("task_type", "unknown"),
                    "task_id": data["metadata"].get("task_id", "unknown"),
                    "score": data["result"].get("solution_correctness_score", 0),
                    "evaluator": data["metadata"].get("evaluator", "unknown"),
                    "annotator": data["metadata"].get("annotator", "unknown")
                }
                evaluations.append(eval_data)
        except (json.JSONDecodeError, KeyError) as e:
            print(f"Warning: Could not parse {json_file}: {e}")
    
    return evaluations

def calculate_domain_performance(evaluations: list) -> pd.DataFrame:
    """Calculate performance by model and domain (task_type)."""
    results = []
    
    # Group by model and domain
    grouped = defaultdict(lambda: defaultdict(list))
    for eval_data in evaluations:
        model = eval_data["model_name"]
        domain = eval_data["task_type"].replace("_task", "")  # Remove "_task" suffix
        score = eval_data["score"]
        grouped[model][domain].append(score)
    
    # Calculate performance metrics
    for model, domains in grouped.items():
        for domain, scores in domains.items():
            total_tasks = len(scores)
            if total_tasks > 0:
                # Count scores 4 and 5 as correct
                correct_tasks = sum(1 for score in scores if score >= 4)
                success_rate = (correct_tasks / total_tasks) * 100
                avg_score = np.mean(scores)
                
                results.append({
                    "model": model,
                    "domain": domain,
                    "total_tasks": total_tasks,
                    "correct_tasks": correct_tasks,
                    "success_rate": success_rate,
                    "average_score": avg_score,
                    "scores": scores
                })
    
    return pd.DataFrame(results)

def calculate_overall_performance(evaluations: list) -> pd.DataFrame:
    """Calculate overall performance ranking for all models."""
    results = []
    
    # Group by model
    grouped = defaultdict(list)
    for eval_data in evaluations:
        model = eval_data["model_name"]
        score = eval_data["score"]
        grouped[model].append(score)
    
    # Calculate overall metrics
    for model, scores in grouped.items():
        total_tasks = len(scores)
        if total_tasks > 0:
            correct_tasks = sum(1 for score in scores if score >= 4)
            success_rate = (correct_tasks / total_tasks) * 100
            avg_score = np.mean(scores)
            
            results.append({
                "model": model,
                "total_tasks": total_tasks,
                "correct_tasks": correct_tasks,
                "success_rate": success_rate,
                "average_score": avg_score
            })
    
    # Sort by success rate
    df = pd.DataFrame(results)
    df = df.sort_values("success_rate", ascending=False).reset_index(drop=True)
    df["rank"] = df.index + 1
    
    return df

def create_overall_model_figure(overall_df: pd.DataFrame):
    """Create a clean bar chart showing overall model performance."""
    
    # Professional typography settings
    plt.rcParams['font.family'] = 'sans-serif'
    plt.rcParams['font.sans-serif'] = ['Arial', 'Helvetica', 'DejaVu Sans']
    plt.rcParams['font.size'] = 11
    plt.rcParams['axes.labelsize'] = 13
    plt.rcParams['axes.titlesize'] = 15
    plt.rcParams['xtick.labelsize'] = 10
    plt.rcParams['ytick.labelsize'] = 11
    
    # Create figure with white background
    fig, ax = plt.subplots(figsize=(14, 8), facecolor='white')
    ax.set_facecolor('white')
    
    # Data is already sorted by success rate
    models = overall_df["model"].values
    success_rates = overall_df["success_rate"].values
    
    # Option 1: Use gradient colors based on success rate (shows performance visually)
    colors = [SUCCESS_CMAP(rate/100) for rate in success_rates]
    
    # Option 2: Use fixed model colors for consistency (uncomment to use)
    # colors = [MODEL_COLORS.get(model, MAIN_PALETTE[i % len(MAIN_PALETTE)])
    #           for i, model in enumerate(models)]
    
    # Create VERTICAL bar chart
    x_pos = np.arange(len(models))
    bars = ax.bar(x_pos, success_rates, color=colors,
                  edgecolor='#333333', linewidth=1.2, width=0.7,
                  alpha=0.85)
    
    # Set labels and title
    ax.set_xlabel('Model', fontsize=13, color='#333333', fontweight='bold')
    ax.set_ylabel('Success Rate', fontsize=13, color='#333333', fontweight='bold')
    ax.set_title('Overall Model Performance Ranking', 
                fontsize=16, fontweight='bold', pad=20, color='#333333')
    
    # Set x-axis
    ax.set_xticks(x_pos)
    ax.set_xticklabels(models, rotation=45, ha='right')
    
    # Set y-axis (0-100%)
    ax.set_ylim(0, 105)
    ax.set_yticks(np.arange(0, 101, 20))
    ax.set_yticklabels([f'{int(y)}%' for y in np.arange(0, 101, 20)])
    
    # Add subtle grid
    ax.grid(axis='y', alpha=0.2, linestyle='--', linewidth=0.5)
    ax.set_axisbelow(True)
    
    # Remove top and right spines
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_linewidth(0.5)
    ax.spines['bottom'].set_linewidth(0.5)
    
    # Add value labels
    for i, (bar, row) in enumerate(zip(bars, overall_df.itertuples())):
        rate = row.success_rate
        # Success rate on top of bar
        ax.text(i, rate + 1, f'{rate:.2f}%', 
               ha='center', va='bottom', fontsize=10, 
               fontweight='bold', color='#333333')
        
        # Removed rank labels for cleaner look
    
    plt.tight_layout()
    
    # Save figure
    figures_dir = Path(__file__).parent / "figures"
    figures_dir.mkdir(parents=True, exist_ok=True)  # Ensure directory exists
    
    # Save PNG
    output_path_png = figures_dir / "overall_model_ranking.png"
    plt.savefig(output_path_png, dpi=300, bbox_inches='tight',
               facecolor='white', edgecolor='none')
    
    # Save EPS
    output_path_eps = figures_dir / "overall_model_ranking.eps"
    plt.savefig(output_path_eps, format='eps', bbox_inches='tight',
               facecolor='white', edgecolor='none')
    
    plt.close()
    
    print(f"ðŸ“Š Overall model ranking figure saved to: {output_path_png} and {output_path_eps}")

def create_overall_domain_figure(domain_df: pd.DataFrame):
    """Create a clean bar chart showing overall domain performance."""
    
    # Professional typography settings
    plt.rcParams['font.family'] = 'sans-serif'
    plt.rcParams['font.sans-serif'] = ['Arial', 'Helvetica', 'DejaVu Sans']
    plt.rcParams['font.size'] = 11
    plt.rcParams['axes.labelsize'] = 13
    plt.rcParams['axes.titlesize'] = 15
    plt.rcParams['xtick.labelsize'] = 11
    plt.rcParams['ytick.labelsize'] = 11
    
    # Create figure with white background
    fig, ax = plt.subplots(figsize=(12, 7), facecolor='white')
    ax.set_facecolor('white')
    
    # Calculate domain statistics
    domain_stats = domain_df.groupby("domain").agg({
        "success_rate": "mean",
        "total_tasks": "sum",
        "correct_tasks": "sum"
    })
    
    # Use the predefined domain order
    domains = [d for d in DOMAIN_ORDER if d in domain_stats.index]
    success_rates = [domain_stats.loc[d, "success_rate"] for d in domains]
    
    # Use domain-specific colors
    colors = [DOMAIN_COLORS.get(d, MAIN_PALETTE[i % len(MAIN_PALETTE)]) 
             for i, d in enumerate(domains)]
    
    # Create vertical bar chart
    x_pos = np.arange(len(domains))
    bars = ax.bar(x_pos, success_rates, color=colors,
                 edgecolor='#333333', linewidth=1.2, width=0.7,
                 alpha=0.85)
    
    # Set labels and title
    ax.set_xlabel('Domain', fontsize=13, color='#333333', fontweight='bold')
    ax.set_ylabel('Average Success Rate', fontsize=13, color='#333333', fontweight='bold')
    ax.set_title('Domain Difficulty Analysis', 
                fontsize=16, fontweight='bold', pad=20, color='#333333')
    
    # Set x-axis
    ax.set_xticks(x_pos)
    ax.set_xticklabels([d.capitalize() for d in domains], rotation=0, ha='center')
    
    # Set y-axis (0-100%)
    ax.set_ylim(0, 105)
    ax.set_yticks(np.arange(0, 101, 20))
    ax.set_yticklabels([f'{int(y)}%' for y in np.arange(0, 101, 20)])
    
    # Add subtle grid
    ax.grid(axis='y', alpha=0.2, linestyle='--', linewidth=0.5)
    ax.set_axisbelow(True)
    
    # Remove top and right spines
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_linewidth(0.5)
    ax.spines['bottom'].set_linewidth(0.5)
    
    # Add value labels and difficulty indicators
    for i, (bar, rate, domain) in enumerate(zip(bars, success_rates, domains)):
        # Success rate on top
        ax.text(i, rate + 1, f'{rate:.2f}%', 
               ha='center', va='bottom', fontsize=11,
               fontweight='bold', color='#333333')
        
        # Removed difficulty labels below x-axis for cleaner look
    
    # Removed reference lines and legend for cleaner look
    # Uncomment below if you want reference lines back:
    # ax.axhline(y=70, color='#2E7D32', linestyle=':', alpha=0.3, linewidth=1)
    # ax.axhline(y=40, color='#FFA726', linestyle=':', alpha=0.3, linewidth=1)
    
    # Removed summary statistics for cleaner look
    
    plt.tight_layout()
    
    # Save figure
    figures_dir = Path(__file__).parent / "figures"
    figures_dir.mkdir(parents=True, exist_ok=True)  # Ensure directory exists
    
    # Save PNG
    output_path_png = figures_dir / "overall_domain_difficulty.png"
    plt.savefig(output_path_png, dpi=300, bbox_inches='tight',
               facecolor='white', edgecolor='none')
    
    # Save EPS
    output_path_eps = figures_dir / "overall_domain_difficulty.eps"
    plt.savefig(output_path_eps, format='eps', bbox_inches='tight',
               facecolor='white', edgecolor='none')
    
    plt.close()
    
    print(f"ðŸ“Š Overall domain difficulty figure saved to: {output_path_png} and {output_path_eps}")

def create_model_comparison_grid(domain_df: pd.DataFrame, overall_df: pd.DataFrame):
    """Create a 2x3 subplot figure showing performance across domains for all 6 models."""
    
    # Save directly to figures directory (no models subdirectory)
    figures_dir = Path(__file__).parent / "figures"
    
    # Professional typography settings
    plt.rcParams['font.family'] = 'sans-serif'
    plt.rcParams['font.sans-serif'] = ['Arial', 'Helvetica', 'DejaVu Sans']
    plt.rcParams['font.size'] = 9
    plt.rcParams['axes.labelsize'] = 10
    plt.rcParams['axes.titlesize'] = 11
    plt.rcParams['xtick.labelsize'] = 9
    plt.rcParams['ytick.labelsize'] = 9
    
    # Get unique models 
    unique_models = overall_df["model"].unique()
    
    # Create 2x3 subplot figure for up to 6 models
    fig, axes = plt.subplots(2, 3, figsize=(18, 10), facecolor='white')
    fig.suptitle('Model Performance Comparison by Domain', 
                 fontsize=16, fontweight='bold', y=1.02)
    
    # Flatten axes for easier iteration
    axes = axes.flatten()
    
    # Plot all models (up to 6)
    models_to_plot = unique_models[:min(6, len(unique_models))]
    
    for idx, model in enumerate(models_to_plot):
        ax = axes[idx]
        ax.set_facecolor('white')
        
        model_data = domain_df[domain_df["model"] == model]
        
        if model_data.empty:
            ax.axis('off')  # Hide empty subplot
            continue
        
        # Use predefined domain order
        domains_list = []
        success_rates_list = []
        for domain in DOMAIN_ORDER:
            domain_data = model_data[model_data["domain"] == domain]
            if not domain_data.empty:
                domains_list.append(domain)
                success_rates_list.append(domain_data["success_rate"].values[0])
        
        domains = np.array(domains_list)
        success_rates = np.array(success_rates_list)
        
        # Use domain-specific colors for consistency
        colors = [DOMAIN_COLORS.get(d, MAIN_PALETTE[i % len(MAIN_PALETTE)]) 
                  for i, d in enumerate(domains)]
        
        # Create bar chart (vertical bars for domains)
        x_pos = np.arange(len(domains))
        bars = ax.bar(x_pos, success_rates, color=colors, 
                      edgecolor='#333333', linewidth=1.0, width=0.7,
                      alpha=0.85)
        
        # Set labels and title - truncate long model names
        model_display = model if len(model) <= 25 else model[:22] + "..."
        ax.set_title(f'{model_display}', 
                    fontsize=11, fontweight='bold', pad=10, color='#333333')
        
        # Set x-axis
        ax.set_xticks(x_pos)
        ax.set_xticklabels([d.capitalize() for d in domains], rotation=0, ha='center')
        
        # Set y-axis (0-100%)
        ax.set_ylim(0, 105)
        ax.set_yticks(np.arange(0, 101, 20))
        ax.set_yticklabels([f'{int(y)}%' for y in np.arange(0, 101, 20)])
        
        # Add labels only for left and bottom subplots
        if idx in [0, 3]:  # Left column (for 2x3 grid)
            ax.set_ylabel('Success Rate', fontsize=10, color='#333333')
        if idx in [3, 4, 5]:  # Bottom row (for 2x3 grid)
            ax.set_xlabel('Domain', fontsize=10, color='#333333')
        
        # Add subtle grid
        ax.grid(axis='y', alpha=0.2, linestyle='--', linewidth=0.5)
        ax.set_axisbelow(True)
        
        # Remove top and right spines for cleaner look
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
        ax.spines['left'].set_linewidth(0.5)
        ax.spines['bottom'].set_linewidth(0.5)
        
        # Add value labels on top of bars
        for i, (bar, rate) in enumerate(zip(bars, success_rates)):
            # Display success rate percentage
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                   f'{rate:.2f}%', ha='center', va='bottom',
                   fontsize=8, fontweight='bold', color='#333333')
    
    # Hide any unused subplots
    for idx in range(len(models_to_plot), 6):
        axes[idx].axis('off')
    
    plt.tight_layout()
    
    # Save the combined figure directly to figures directory
    # Save PNG
    output_path_png = figures_dir / "models_comparison_2x3.png"
    plt.savefig(output_path_png, dpi=300, bbox_inches='tight', 
               facecolor='white', edgecolor='none')
    
    # Save EPS
    output_path_eps = figures_dir / "models_comparison_2x3.eps"
    plt.savefig(output_path_eps, format='eps', bbox_inches='tight', 
               facecolor='white', edgecolor='none')
    
    plt.close()
    
    print(f"  ðŸ“Š Created 2x3 comparison plot: {output_path_png} and {output_path_eps}")

def print_detailed_results(domain_df: pd.DataFrame, overall_df: pd.DataFrame):
    """Print comprehensive statistics in table-friendly format for paper."""
    
    print("\n" + "="*100)
    print("ðŸŽ¯ VMEVALKIT COMPREHENSIVE EVALUATION STATISTICS")
    print("="*100)
    
    # Table 1: Overall Model Performance Ranking
    print("\nðŸ“Š TABLE 1: OVERALL MODEL PERFORMANCE RANKING")
    print("-" * 100)
    print(f"{'Rank':<6} {'Model':<30} {'Success Rate':<15} {'Avg Score':<12} {'Std Dev':<10}")
    print("-" * 100)
    
    for _, row in overall_df.iterrows():
        # Calculate std dev for this model
        model_scores = []
        for _, domain_row in domain_df[domain_df["model"] == row["model"]].iterrows():
            model_scores.extend(domain_row["scores"])
        std_dev = np.std(model_scores) if model_scores else 0
        
        print(f"{row['rank']:<6} {row['model']:<30} {row['success_rate']:>6.2f}%{'':<8} "
              f"{row['average_score']:>6.3f}{'':<6} {std_dev:>6.3f}")
    
    # Table 2: Domain-wise Performance Matrix
    print("\n\nðŸ“Š TABLE 2: MODEL PERFORMANCE BY DOMAIN (Success Rate %)")
    print("-" * 100)
    
    pivot_table = domain_df.pivot(index="model", columns="domain", values="success_rate")
    
    # Print header
    header = f"{'Model':<30}"
    # Use the predefined domain order
    ordered_domains = [d for d in DOMAIN_ORDER if d in pivot_table.columns]
    for domain in ordered_domains:
        header += f" {domain.capitalize():<12}"
    header += f" {'Average':<12}"
    print(header)
    print("-" * 100)
    
    # Print data
    for model in overall_df["model"]:
        row_str = f"{model:<30}"
        domain_scores = []
        for domain in ordered_domains:
            if model in pivot_table.index and domain in pivot_table.columns:
                value = pivot_table.loc[model, domain]
                if pd.notna(value):
                    row_str += f" {value:>6.1f}%{'':<5}"
                    domain_scores.append(value)
                else:
                    row_str += f" {'N/A':<12}"
            else:
                row_str += f" {'N/A':<12}"
        
        # Add average
        if domain_scores:
            avg_score = np.mean(domain_scores)
            row_str += f" {avg_score:>6.1f}%"
        else:
            row_str += f" {'N/A':<12}"
        print(row_str)
    
    # Table 3: Domain Statistics
    print("\n\nðŸ“Š TABLE 3: DOMAIN-LEVEL STATISTICS")
    print("-" * 100)
    print(f"{'Domain':<15} {'Avg Success':<15} {'Std Dev':<12} {'Min Score':<12} {'Max Score':<12} {'Total Tasks':<12} {'Difficulty':<12}")
    print("-" * 100)
    
    domain_stats = domain_df.groupby("domain").agg({
        "success_rate": ["mean", "std", "min", "max"],
        "total_tasks": "sum"
    }).round(2)
    
    # Use the predefined domain order
    for domain in DOMAIN_ORDER:
        if domain not in domain_stats.index:
            continue
        avg_rate = domain_stats.loc[domain, ("success_rate", "mean")]
        std_rate = domain_stats.loc[domain, ("success_rate", "std")]
        min_rate = domain_stats.loc[domain, ("success_rate", "min")]
        max_rate = domain_stats.loc[domain, ("success_rate", "max")]
        total_tasks = int(domain_stats.loc[domain, ("total_tasks", "sum")])
        
        difficulty = "Easy" if avg_rate > 70 else "Medium" if avg_rate > 40 else "Hard"
        
        print(f"{domain.capitalize():<15} {avg_rate:>6.2f}%{'':<8} {std_rate:>8.2f}{'':<4} "
              f"{min_rate:>6.2f}%{'':<5} {max_rate:>6.2f}%{'':<5} {total_tasks:>8}{'':<4} {difficulty:<12}")
    
    # Table 4: Score Distribution Analysis
    print("\n\nðŸ“Š TABLE 4: SCORE DISTRIBUTION ANALYSIS")
    print("-" * 100)
    
    all_scores = []
    score_by_model = defaultdict(list)
    for _, row in domain_df.iterrows():
        all_scores.extend(row["scores"])
        score_by_model[row["model"]].extend(row["scores"])
    
    score_counts = np.bincount(all_scores, minlength=6)[1:]  # Scores 1-5
    total_scores = len(all_scores)
    
    print(f"{'Score':<10} {'Count':<10} {'Percentage':<15} {'Cumulative %':<15} {'Classification':<20}")
    print("-" * 100)
    
    cumulative = 0
    for i, count in enumerate(score_counts, 1):
        percentage = (count / total_scores) * 100 if total_scores > 0 else 0
        cumulative += percentage
        classification = "Success" if i >= 4 else "Failure"
        print(f"Score {i:<4} {count:<10} {percentage:>6.2f}%{'':<8} {cumulative:>6.2f}%{'':<8} {classification:<20}")
    
    # Table 5: Model-Domain Detailed Breakdown
    print("\n\nðŸ“Š TABLE 5: DETAILED MODEL-DOMAIN BREAKDOWN")
    print("-" * 100)
    print(f"{'Model':<30} {'Domain':<15} {'Tasks':<8} {'Correct':<10} {'Success%':<12} {'Avg Score':<12}")
    print("-" * 100)
    
    for model in sorted(domain_df["model"].unique()):
        model_data = domain_df[domain_df["model"] == model]
        first_row = True
        # Use the predefined domain order
        for domain in DOMAIN_ORDER:
            domain_row = model_data[model_data["domain"] == domain]
            if domain_row.empty:
                continue
            row = domain_row.iloc[0]
            model_name = model if first_row else ""
            first_row = False
            print(f"{model_name:<30} {domain.capitalize():<15} {row['total_tasks']:<8} "
                  f"{row['correct_tasks']:<10} {row['success_rate']:>6.2f}%{'':<5} {row['average_score']:>6.3f}")
        
        # Add model summary
        model_overall = overall_df[overall_df["model"] == model].iloc[0]
        print(f"{'  TOTAL':<30} {'All Domains':<15} {model_overall['total_tasks']:<8} "
              f"{model_overall['correct_tasks']:<10} {model_overall['success_rate']:>6.2f}%{'':<5} "
              f"{model_overall['average_score']:>6.3f}")
        print("-" * 100)
    
    # Table 6: Statistical Summary
    print("\n\nðŸ“Š TABLE 6: STATISTICAL SUMMARY")
    print("-" * 100)
    
    print(f"Total Evaluations: {total_scores:,}")
    print(f"Total Correct (Scores 4-5): {sum(score_counts[3:]):,}")
    print(f"Total Failed (Scores 1-3): {sum(score_counts[:3]):,}")
    print(f"Overall Success Rate: {(sum(score_counts[3:]) / total_scores * 100):.2f}%")
    print(f"Number of Models Evaluated: {len(overall_df)}")
    print(f"Number of Domains: {len(domain_df['domain'].unique())}")
    print(f"Average Tasks per Model: {total_scores / len(overall_df):.1f}")
    print(f"Average Tasks per Domain: {total_scores / len(domain_df['domain'].unique()):.1f}")
    
    # Best performers
    print("\nðŸ† TOP PERFORMERS:")
    print("-" * 50)
    
    # Best overall model
    best_model = overall_df.iloc[0]
    print(f"Best Overall Model: {best_model['model']} ({best_model['success_rate']:.2f}%)")
    
    # Best model per domain
    for domain in DOMAIN_ORDER:
        if domain in domain_df["domain"].unique():
            domain_best = domain_df[domain_df["domain"] == domain].nlargest(1, "success_rate").iloc[0]
            print(f"Best in {domain.capitalize()}: {domain_best['model']} ({domain_best['success_rate']:.2f}%)")
    
    # Difficulty analysis
    print("\nðŸ“Š DOMAIN PERFORMANCE (in order: Sudoku â†’ Raven â†’ Rotation â†’ Maze â†’ Chess):")
    print("-" * 50)
    
    domain_difficulty = domain_df.groupby("domain")["success_rate"].mean()
    
    # Use the predefined domain order
    for domain in DOMAIN_ORDER:
        if domain in domain_difficulty.index:
            avg_rate = domain_difficulty[domain]
            difficulty_level = "Easy" if avg_rate > 70 else "Medium" if avg_rate > 40 else "Hard"
            print(f"{domain.capitalize():<15} - Average Success: {avg_rate:>6.2f}% ({difficulty_level})")
    
    # Table 7: Performance Variance Analysis
    print("\n\nðŸ“Š TABLE 7: PERFORMANCE VARIANCE ANALYSIS")
    print("-" * 100)
    print(f"{'Model':<30} {'Min Domain %':<15} {'Max Domain %':<15} {'Variance':<12} {'Consistency':<15}")
    print("-" * 100)
    
    for model in overall_df["model"]:
        model_domain_scores = domain_df[domain_df["model"] == model]["success_rate"].values
        if len(model_domain_scores) > 0:
            min_score = model_domain_scores.min()
            max_score = model_domain_scores.max()
            variance = max_score - min_score
            consistency = "High" if variance < 20 else "Medium" if variance < 40 else "Low"
            
            print(f"{model:<30} {min_score:>6.2f}%{'':<8} {max_score:>6.2f}%{'':<8} "
                  f"{variance:>6.2f}{'':<6} {consistency:<15}")
    
    print("\n" + "="*100)
    print("END OF COMPREHENSIVE STATISTICS")
    print("="*100)

def main():
    parser = argparse.ArgumentParser(
        description="Analyze VMEvalKit evaluation results",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python analysis/plot.py --eval-folder data/evaluations/human-eval/
  python analysis/plot.py --eval-folder data/evaluations/gpt4o-eval/
        """
    )
    
    parser.add_argument("--eval-folder", required=True, type=str,
                      help="Path to evaluation folder (e.g., data/evaluations/human-eval/)")
    
    args = parser.parse_args()
    
    eval_folder = Path(args.eval_folder)
    if not eval_folder.exists():
        print(f"âŒ Error: Evaluation folder not found: {eval_folder}")
        return
    
    # Load and analyze data
    print(f"ðŸ“‚ Loading evaluations from: {eval_folder}")
    evaluations = load_evaluation_data(eval_folder)
    
    if not evaluations:
        print(f"âŒ No evaluation files found in {eval_folder}")
        return
    
    print(f"âœ… Loaded {len(evaluations)} evaluations")
    
    # Calculate performance metrics
    domain_df = calculate_domain_performance(evaluations)
    overall_df = calculate_overall_performance(evaluations)
    
    # Print detailed results
    print_detailed_results(domain_df, overall_df)
    
    # Create visualizations
    print(f"\nðŸ“Š Creating clean bar chart visualizations...")
    
    # 1. Overall model ranking figure
    create_overall_model_figure(overall_df)
    
    # 2. Overall domain difficulty figure
    create_overall_domain_figure(domain_df)
    
    # 3. Model comparison grid (2x3)
    print(f"\nðŸ“Š Creating model comparison grid...")
    create_model_comparison_grid(domain_df, overall_df)

if __name__ == "__main__":
    main()