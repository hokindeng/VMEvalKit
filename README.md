# VMEvalKit ğŸ¥ğŸ§ 

Evaluate reasoning capabilities in video generation models through cognitive tasks.

## Overview

VMEvalKit tests whether video models can solve visual problems (mazes, chess, puzzles) by generating solution videos. 

**Key requirement**: Models must accept BOTH:
- ğŸ“¸ An input image (the problem)
- ğŸ“ A text prompt (instructions)

## Installation

```bash
git clone https://github.com/yourusername/VMEvalKit.git
cd VMEvalKit
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
pip install -e .
```

## Quick Start

```python
from vmevalkit.runner.inference import InferenceRunner

# Initialize runner with structured output
runner = InferenceRunner(output_dir="output")

# Generate video solution
result = runner.run(
    model_name="luma-ray-2",
    image_path="data/questions/maze_task/maze_0000/first_frame.png",
    text_prompt="Navigate the green dot through the maze corridors to reach the red flag"
)

print(f"Video saved to: {result['inference_dir']}")
# Each inference creates a self-contained folder with:
# - video/: Generated video file
# - question/: Input images and prompt  
# - metadata.json: Complete inference metadata
```

## Supported Models

VMEvalKit supports **36+ models** across **9 providers**:

**Commercial APIs (28 models):**
- **Luma Dream Machine**: 2 models (`luma-ray-2`, `luma-ray-flash-2`)
- **Google Veo**: 3 models (`veo-2.0-generate`, `veo-3.0-generate`, etc.)
- **WaveSpeed WAN**: 18 models (2.1 & 2.2 variants)
- **Runway ML**: 3 models
- **OpenAI Sora**: 2 models

**Open-Source Models (8 models):**
- **LTX-Video**: 3 models (13B distilled, 13B dev, 2B distilled)
- **HunyuanVideo**: 1 model (high-quality 720p)
- **VideoCrafter**: 1 model (text-guided generation)
- **DynamiCrafter**: 3 models (256p, 512p, 1024p)

All models support **image + text â†’ video** for reasoning evaluation.

## Core Concepts

### Task Pair: The Fundamental Unit
Every VMEvalKit dataset consists of **Task Pairs** - the basic unit for video reasoning evaluation:

- ğŸ“¸ **Initial state image** (`first_frame.png` - the reasoning problem)
- ğŸ¯ **Final state image** (`final_frame.png` - the solution/goal state)  
- ğŸ“ **Text prompt** (`prompt.txt` - instructions for video model)
- ğŸ“Š **Rich metadata** (`question_metadata.json` - difficulty, task-specific parameters, etc.)

Each task pair is organized in its own folder (`data/questions/{domain}_task/{question_id}/`) containing all four files. Models must generate videos showing the reasoning process from initial â†’ final state.

## Tasks

- **Maze Solving**: Navigate from start to finish
- **Mental Rotation**: Rotate 3D objects to match targets
- **Chess Puzzles**: Demonstrate puzzle solutions
- **Raven's Matrices**: Complete visual patterns

## Configuration

Create `.env`:
```bash
LUMA_API_KEY=your_key_here
AWS_ACCESS_KEY_ID=your_aws_key
AWS_SECRET_ACCESS_KEY=your_aws_secret
S3_BUCKET=vmevalkit
AWS_DEFAULT_REGION=us-east-2
```

## Project Structure

```
VMEvalKit/
â”œâ”€â”€ vmevalkit/
â”‚   â”œâ”€â”€ runner/         # Inference runners
â”‚   â”œâ”€â”€ models/         # Model implementations
â”‚   â”œâ”€â”€ core/           # Evaluation framework
â”‚   â”œâ”€â”€ tasks/          # Task definitions
â”‚   â””â”€â”€ utils/          # Utilities
â”œâ”€â”€ data/
â”‚   â””â”€â”€ questions/      # Dataset with per-question folders
â”‚       â”œâ”€â”€ vmeval_dataset.json  # Master dataset manifest
â”‚       â”œâ”€â”€ chess_task/          # Chess reasoning questions
â”‚       â”‚   â””â”€â”€ chess_0000/      # Individual question folder
â”‚       â”‚       â”œâ”€â”€ first_frame.png
â”‚       â”‚       â”œâ”€â”€ final_frame.png
â”‚       â”‚       â”œâ”€â”€ prompt.txt
â”‚       â”‚       â””â”€â”€ question_metadata.json
â”‚       â”œâ”€â”€ maze_task/           # Maze navigation questions
â”‚       â”œâ”€â”€ raven_task/          # Pattern completion questions
â”‚       â””â”€â”€ rotation_task/       # 3D rotation questions
â”œâ”€â”€ output/             # Structured inference outputs
â”‚   â””â”€â”€ <inference_id>/ # Self-contained folders per inference
â”‚       â”œâ”€â”€ video/      # Generated video file
â”‚       â”œâ”€â”€ question/   # Input images and prompt
â”‚       â””â”€â”€ metadata.json # Complete inference metadata
â”œâ”€â”€ examples/           # Example scripts
â””â”€â”€ tests/              # Unit tests
```

## Structured Output System

Each inference creates a **self-contained folder** with all relevant data:

```
output/<model>_<question_id>_<timestamp>/
â”œâ”€â”€ video/
â”‚   â””â”€â”€ generated_video.mp4    # Output video
â”œâ”€â”€ question/
â”‚   â”œâ”€â”€ first_frame.png        # Input image (sent to model)
â”‚   â”œâ”€â”€ final_frame.png        # Reference image (not sent)
â”‚   â”œâ”€â”€ prompt.txt             # Text prompt used
â”‚   â””â”€â”€ question_metadata.json # Full question data from dataset
â””â”€â”€ metadata.json              # Complete inference metadata
```

This structure ensures reproducibility and makes batch analysis easy.

## Examples

See `examples/experiment_2025-10-14.py` for sequential inference across multiple models.

## Submodules

Initialize after cloning:
```bash
git submodule update --init --recursive
```

- **maze-dataset**: Maze datasets for ML evaluation
- **HunyuanVideo-I2V**: High-quality image-to-video generation (720p)
- **LTX-Video**: Real-time video generation models
- **VideoCrafter**: Text-guided video generation
- **DynamiCrafter**: Image animation with video diffusion

## Contributing

### Adding New Models

VMEvalKit supports 36+ models across 9 providers and is designed to easily accommodate new models.

**Requirements:**
- Model must support **both image + text input** for reasoning evaluation
- Follow the unified inference interface

**Quick Steps:**
1. Create wrapper class in `vmevalkit/models/{provider}_inference.py`
2. Register in `vmevalkit/runner/inference.py` 
3. Update imports in `vmevalkit/models/__init__.py`

**Documentation:**
- ğŸ“š **Complete Guide**: [docs/ADDING_MODELS.md](docs/ADDING_MODELS.md)

Both API-based and open-source (submodule) integration patterns are supported.

## Running Experiments

### Quick Start

Generate dataset and run experiments:

```bash
cd /Users/access/VMEvalKit
source venv/bin/activate

# Generate dataset (if needed)
python -m vmevalkit.runner.create_dataset --pairs-per-domain 15

# Run experiment (1 task per domain for testing)
python examples/experiment_2025-10-14.py

# Run all tasks
python examples/experiment_2025-10-14.py --all-tasks
```

### Resume Mechanism

The experiment script includes robust resume capability for long-running experiments:

**Features:**
- ğŸ”„ Sequential execution: one model at a time, one task at a time
- âš¡ Automatic checkpointing every 5 completed jobs
- ğŸ›¡ï¸ Graceful interruption handling (Ctrl+C saves progress)
- ğŸ“¥ Resume from latest or specific experiment
- ğŸ“Š Track completed, failed, and in-progress jobs

**Usage:**

```bash
# Resume latest interrupted experiment
python examples/experiment_2025-10-14.py --resume latest

# Resume specific experiment
python examples/experiment_2025-10-14.py --resume experiment_20241016_143022

# List available checkpoints
python examples/experiment_2025-10-14.py --list-checkpoints

# Start with custom experiment ID
python examples/experiment_2025-10-14.py --experiment-id my_test_001
```

**Command Options:**

| Option | Description |
|--------|-------------|
| `--resume <ID or 'latest'>` | Resume a previous experiment |
| `--no-resume` | Disable resume mechanism |
| `--experiment-id <ID>` | Set custom experiment ID |
| `--all-tasks` | Run all tasks instead of 1 per domain |
| `--list-checkpoints` | List available checkpoints |

**How It Works:**
- Progress saved to `data/outputs/pilot_experiment/logs/checkpoint_*.json`
- Completed jobs won't be re-run on resume
- Failed jobs can be retried
- Interrupted jobs are automatically retried

## License

MIT