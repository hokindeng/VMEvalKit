# VMEvalKit Scoring Module

This module provides comprehensive scoring methods for assessing video generation models' reasoning capabilities, along with powerful analysis tools for processing and visualizing results.

## Table of Contents
- [Available Scorers](#available-scorers)
- [Analysis Tools](#analysis-tools)
- [Scoring Criteria](#scoring-criteria)
- [Output Structure](#output-structure)
- [Command-Line Interface](#command-line-interface)
- [Custom Scorers](#custom-scorers)
- [API Reference](#api-reference)
- [Resume Capability](#resume-capability)
- [Analysis Workflow](#analysis-workflow)
- [Tips and Best Practices](#tips-and-best-practices)

## Available Scorers

### 1. Human Scorer
Interactive web interface for human scoring of generated videos.

**Features:**
- Gradio-based web interface
- Side-by-side display of input and generated video
- Structured scoring criteria
- Real-time progress tracking with model-specific statistics
- Export results as JSON
- **Automatic skip of already scored tasks** (checks scorings folder)
- **In-interface annotator name input** (no command-line setup needed)
- Visual queue status with completion percentages
- Refresh capability to reload scoring queue

**Usage:**
```bash
# Run human scoring interface (scores entire pilot experiment)
# Automatically skips tasks that have any existing scoring
python examples/score_videos.py human

# Or use the command-line runner
python -m vmevalkit.runner.score human --annotator "John Doe" --port 7860 --share
```

When launched, the interface will:
- Prompt for the annotator name
- Automatically skip tasks with existing scorings
- Show progress and statistics by model
- Display completion percentages

**Python Usage:**
```python
from vmevalkit.eval import HumanScorer

scorer = HumanScorer(
    experiment_name="pilot_experiment"
)
# Automatically skips tasks with existing scorings in data/scorings/
# Annotator name is set within the Gradio interface

scorer.launch_interface(share=True, port=7860)
```

### 2. GPT-4O Scorer
Automatic scoring using OpenAI's GPT-4O vision model.

**Features:**
- Compares final frame of generated video with ground truth
- Direct assessment of whether the model answered the question correctly
- **Task-specific scoring prompts** for each domain
- Batch processing of all models with async execution
- Detailed scoring and explanations
- **Resume capability** - saves after each task scoring
- Progress logging with skip statistics

**Task-Specific Guidance:**
- **Chess**: Checks if final board position matches expected position after correct move
- **Maze**: Verifies complete path from start to end matches expected solution
- **Rotation**: Checks if final rotation angle and position match expected result
- **Raven**: Verifies pattern completion matches expected pattern
- **Sudoku**: Checks if numbers placed match expected solution

**Usage:**
```bash
# Evaluate entire pilot experiment (all models, all tasks)
python examples/score_videos.py gpt4o

# Or use the command-line runner with specific options
python -m vmevalkit.runner.score gpt4o --experiment pilot_experiment --temperature 0.1
```

**Requirements:**
- Set `OPENAI_API_KEY` environment variable
- Install dependencies: `opencv-python`, `httpx`

## Analysis Tools

### Performance Visualization (`analysis/plot.py`)
Creates professional visualizations of scoring results.

**Features:**
- Overall model performance ranking charts
- Domain-specific performance heatmaps
- Score distribution analysis
- Binary success rate calculation (scores 4-5 = success)
- Sophisticated color schemes and typography
- Export to PNG and EPS formats

**Usage:**
```bash
# Analyze human scoring results
python analysis/plot.py --eval-folder data/scorings/human-eval/

# Analyze GPT-4O scoring results
python analysis/plot.py --eval-folder data/scorings/gpt4o-eval/
```

**Outputs:**
1. **Overall Performance Bar Chart**: Shows success rates for all models
2. **Domain Performance Heatmap**: Model × Domain matrix visualization
3. **Score Distribution Plots**: Distribution of scores per model
4. **Detailed Statistics CSV**: Complete performance metrics

### Statistical Comparison Tool (`analysis/stats.py`)
Comprehensive statistical analysis comparing GPT-4O and human scorings.

**Features:**
- Paired t-test and Wilcoxon signed-rank test
- Multiple correlation analyses (Pearson, Spearman, Kendall)
- Cohen's kappa for inter-rater reliability
- Bootstrap confidence intervals
- Convergence analysis to find equivalence threshold
- Task-type specific analysis
- Publication-ready scatter plots

**Usage:**
```bash
# Run full statistical comparison
python analysis/stats.py
```

**Statistical Tests Performed:**
1. **Basic Statistics**: Mean, std, median for both scorers
2. **Paired T-Test**: Tests if means are significantly different
3. **Wilcoxon Test**: Non-parametric alternative
4. **Correlation Analysis**: Measures agreement strength
5. **Inter-rater Reliability**: Cohen's kappa coefficient
6. **Bootstrap CI**: 95% confidence intervals for mean difference
7. **Convergence Analysis**: Finds sample size for statistical equivalence

## Scoring Criteria

### Scoring Scale (1-5)
Both scorers use a 1-5 scale for solution correctness:

**Solution Correctness Score**: 
- **1**: Completely wrong solution - no understanding of task
- **2**: Mostly incorrect with minor correct elements
- **3**: Partially correct (about half correct)
- **4**: Mostly correct with minor errors ✓
- **5**: Perfect solution ✓

### Binary Grading System
**Important**: For final analysis, scores are converted to binary:
- **Success (Correct)**: Scores 4 and 5
- **Failure (Incorrect)**: Scores 1, 2, and 3

This binary system is used in all performance calculations and visualizations.

## Output Structure

### Directory Structure
Scorings are saved in `data/scorings/` following this structure:
```
data/scorings/
├── human-eval/              # Human scorings
│   └── pilot_experiment/
│       ├── luma-ray-2/
│       │   ├── chess_task/
│       │   │   ├── chess_0000/
│       │   │   │   └── human-eval.json
│       │   │   └── ...
│       │   └── ...
│       └── ...
├── gpt4o-eval/              # GPT-4O scorings  
│   └── pilot_experiment/
│       ├── GPT4OScorer_all_models.json  # Aggregated results
│       └── [same structure as human-eval]
└── custom-eval/             # Custom scorings
```

### JSON File Format
Each scoring JSON file contains:
```json
{
  "metadata": {
    "scorer": "human-eval",        // or "GPT4OScorer"
    "annotator": "John Doe",           // (human only)
    "timestamp": "2024-10-14T12:00:00",
    "model_name": "luma-ray-2",
    "task_type": "chess_task",
    "task_id": "chess_0000"
  },
  "result": {
    "solution_correctness_score": 5,  // 1-5 scale
    "explanation": "Perfect solution", // (GPT-4O only)
    "comments": "User comments",      // (human only)
    "scoring_type": "final_frame_comparison", // (GPT-4O only)
    "status": "completed"              // or "failed", "skipped"
  }
}
```

## Command-Line Interface

### Using the Runner Module
The `vmevalkit.runner.score` module provides a unified CLI:

```bash
# Human scoring with options
python -m vmevalkit.runner.score human \
  --experiment pilot_experiment \
  --annotator "John Doe" \
  --port 7860 \
  --share

# GPT-4O scoring with options
python -m vmevalkit.runner.score gpt4o \
  --experiment pilot_experiment \
  --output-dir data/scorings \
  --temperature 0.1
```

### Environment Variables
```bash
# Required for GPT-4O scoring
export OPENAI_API_KEY="your-api-key"

# Optional: Set default paths
export VMEVAL_DATA_DIR="/path/to/data"
export VMEVAL_OUTPUT_DIR="/path/to/scorings"
```

## Custom Scorers

To create a custom scorer, follow the pattern of existing scorers:

```python
from pathlib import Path
from datetime import datetime
import json

class MyScorer:
    def __init__(self, output_dir="data/scorings/my-eval", 
                 experiment_name="pilot_experiment"):
        self.output_dir = Path(output_dir)
        self.experiment_name = experiment_name
        self.experiment_dir = Path("data/outputs") / experiment_name
    
    def _has_scoring(self, model_name, task_type, task_id):
        """Check if task already scored (for resume capability)."""
        eval_path = self.output_dir / self.experiment_name / model_name / task_type / task_id
        eval_file = eval_path / "MyScorer.json"
        return eval_file.exists()
    
    def score_single(self, model_name, task_type, task_id, video_path):
        # Your scoring logic here
        return {
            "solution_correctness_score": 5,  # 1-5 scale
            "explanation": "The solution perfectly solves the task",
            "status": "completed"
        }
    
    def _save_result(self, model_name, task_type, task_id, eval_result):
        """Save scoring result in standard format."""
        output_path = self.output_dir / self.experiment_name / model_name / task_type / task_id
        output_path.mkdir(parents=True, exist_ok=True)
        
        with open(output_path / "MyScorer.json", 'w') as f:
            json.dump({
                "metadata": {
                    "scorer": "MyScorer",
                    "timestamp": datetime.now().isoformat(),
                    "model_name": model_name,
                    "task_type": task_type,
                    "task_id": task_id
                },
                "result": eval_result
            }, f, indent=2)
    
    def score_model(self, model_name):
        """Evaluate all tasks for a model with resume capability."""
        # Iterate through tasks, skip if already scored
        # Save results using _save_result method
        pass
```

Note: Scorers are standalone classes - no base class required. Ensure your scorer:
1. Saves results in the standard JSON format
2. Implements resume capability via `_has_scoring()` check
3. Uses consistent directory structure
4. Includes proper metadata

## API Reference

### HumanScorer

**Parameters:**
- `output_dir`: Directory for saving scorings (default: "data/scorings/human-eval")
- `experiment_name`: Name of experiment to score (default: "pilot_experiment")

**Methods:**
- `launch_interface(share, port)`: Start web interface with annotator name input
- `_load_scoring_queue()`: Load tasks, skipping already scored ones
- `_get_queue_status_text()`: Generate detailed queue status with model breakdown
- `_save_scoring()`: Save scoring result in standard format

**Automatic Features:**
- Checks for existing `*-eval.json` files
- Automatically skips scored tasks
- Real-time queue status updates
- Model-specific progress tracking

### GPT4OScorer

**Parameters:**
- `output_dir`: Directory for saving scorings (default: "data/scorings/gpt4o-eval")
- `experiment_name`: Name of experiment to score (default: "pilot_experiment")
- `api_key`: OpenAI API key (defaults to OPENAI_API_KEY env var)
- `model`: GPT model to use (default: "gpt-4o")
- `temperature`: Temperature for responses (default: 0.1)

**Methods:**
- `extract_final_frame(video_path)`: Extract the final frame from video
- `create_prompt(task_type)`: Generate task-specific scoring prompts
- `score_single_async()`: Async scoring of single video
- `score_model_async()`: Async scoring of all tasks for a model
- `score_all_models()`: Evaluate all models in experiment
- `_has_scoring()`: Check if task already scored (resume support)
- `_save_single_result()`: Save result immediately for resume capability

**Async Features:**
- Uses httpx for async API calls
- Batch processing with progress logging
- Automatic retry on failures

### ScoringComparator

**Location:** `analysis/stats.py`

**Methods:**
- `load_scorings()`: Load both GPT-4O and human scorings
- `prepare_paired_data()`: Match scorings for comparison
- `basic_statistics()`: Calculate descriptive statistics
- `paired_t_test()`: Test for significant differences
- `correlation_analysis()`: Calculate correlation coefficients
- `inter_rater_reliability()`: Compute Cohen's kappa
- `bootstrap_confidence_intervals()`: Generate confidence intervals
- `convergence_analysis()`: Find statistical equivalence threshold
- `plot_comparisons()`: Create publication-ready visualizations

## Resume Capability

### Human Scoring
- **Automatic Resume**: Skips already scored tasks
- **Detection Method**: Checks for any `*-eval.json` files
- **Progress Tracking**: Shows tasks skipped and remaining
- **Re-scoring**: Manually delete scoring files to re-score

### GPT-4O Scoring
- **Automatic Resume**: Skips already scored tasks (saves after each)
- **Detection Method**: Checks for `GPT4OScorer.json` files
- **Progress Logging**: Reports skipped, scored, and failed counts
- **Interrupt Safe**: Can interrupt with Ctrl+C and resume later

## Analysis Workflow

### Complete Scoring Pipeline
```bash
# 1. Run inference to generate videos
python examples/experiment_2025-10-14.py

# 2. Run human scoring (interactive)
python examples/score_videos.py human

# 3. Run GPT-4O scoring (automatic)
python examples/score_videos.py gpt4o

# 4. Visualize results
python analysis/plot.py --eval-folder data/scorings/human-eval/
python analysis/plot.py --eval-folder data/scorings/gpt4o-eval/

# 5. Compare human vs GPT-4O statistically
python analysis/stats.py

# 6. View generated visualizations
open analysis/statistics/gpt4o_vs_human_comparison.png
open analysis/performance_by_domain.png
```

### Output Files Generated
1. **Scoring JSONs**: Individual task scorings
2. **Performance Charts**: PNG and EPS visualizations
3. **Statistical Results**: JSON with comparison metrics
4. **CSV Reports**: Detailed performance tables

## Tips and Best Practices

### For Human Scoring
- **Session Management**: Complete scorings in one session when possible
- **Annotator Consistency**: Use same annotator for related tasks
- **Comments**: Add detailed comments for edge cases or ambiguous results
- **Progress Monitoring**: Use refresh button to update queue status
- **Backup**: Scorings are saved immediately after submission

### For GPT-4O Scoring
- **Cost Management**: Monitor API usage for large experiments
- **Temperature**: Use low temperature (0.1) for consistency
- **Ground Truth**: Only tasks with final frame ground truth will be scored
- **Batch Processing**: Let it run uninterrupted for efficiency
- **Resume on Failure**: Safe to restart after interruptions

### For Analysis
- **Binary Grading**: Remember scores 4-5 are considered success
- **Statistical Significance**: Use stats.py to validate comparisons
- **Visualization**: Use plot.py for publication-ready figures
- **Domain Analysis**: Check performance variations across task types

### General Best Practices
1. **Order of Operations**: Always run inference → scoring → analysis
2. **Experiment Naming**: Use descriptive experiment names for organization
3. **Version Control**: Track scoring results in git for reproducibility
4. **Documentation**: Document any custom scoring criteria
5. **Validation**: Compare human and GPT-4O results for validation
6. **Error Handling**: Check logs for failed scorings
7. **Data Backup**: Regularly backup scoring results

## Troubleshooting

### Common Issues and Solutions

**Human Scoring Interface Not Loading:**
- Check if port 7860 is available
- Try using `--share` flag for public URL
- Verify Gradio is installed: `pip install gradio`

**GPT-4O API Errors:**
- Verify OPENAI_API_KEY is set correctly
- Check API quota and rate limits
- Ensure opencv-python is installed

**Missing Scorings:**
- Check scoring directory structure
- Verify experiment name matches
- Look for error messages in logs

**Statistical Analysis Failures:**
- Ensure paired scorings exist
- Check for sufficient sample size
- Verify data format consistency